---
title: "SurvMap-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SurvMap-vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette represents and introduction to the use of the package SurvMap. $\frac{m}{n}$ First we will show some aspects linked to matrix the denoising function implemented in the package, that is derived from (10.1109/TIT.2014.2323359). It allows to remove the noise part of matrices that are assumed to be composed by the addition of a signal matrix and a gaussian noise matrix.

```{r setup}
library(SurvMap)
```

Let's first create a signal matrix for testing.

```{r,fig.dim = c(5,7),fig.align = 'center'}
t <- seq(-3,3,0.01)
Utrue <- cbind(cos(17*t) * exp(-t^2), sin(11*t))
Strue <- cbind(c(2,0),c(0,0.5))
Vtrue <- cbind(sin(5*t)* exp(-t^2),cos(13*t))
X <- Utrue %*% Strue %*% t(Vtrue)
image(t(X))
```

Then add some gaussian noise to our matrix with $\sigma = 1$ and $\mu = 0$.

```{r,fig.dim = c(5,7),fig.align = 'center'}
sigma <- 1
noise <- matrix(rnorm(601 * 601),ncol = 601)
Xnoisy <- X + noise
image(t(Xnoisy))
```

Since we will work with rectangular matrices in most applications let's get a rectangular noisy matrix by removing some columns of our original noisy matrix.


```{r,fig.dim = c(5,7),fig.align = 'center'}
Xnoisy_r <- Xnoisy[,1:400]
dim(Xnoisy_r)
image(t(Xnoisy_r))
```

And lets do the same with the  signal matrix.

```{r,fig.dim = c(5,7),fig.align = 'center'}
X_r <- X[,1:400]
dim(X_r)
image(t(X_r))
```

Now let's  denoise our rectangular noisy matrix. 

```{r,fig.dim = c(5,7),fig.align = 'center'}
omega_found <- get_omega(ncol(Xnoisy_r)/nrow(Xnoisy_r))
svd_Xnoisy_r <- svd(Xnoisy_r)
D <- diag(svd_Xnoisy_r$d)
U <- svd_Xnoisy_r$u
V <- svd_Xnoisy_r$v
Xnoisy_reconstructed <- U %*% D %*% t(V)
image(t(Xnoisy_reconstructed))
```

```{r,fig.dim = c(5,7),fig.align = 'center'}
omega_found <- get_omega(ncol(Xnoisy_r)/nrow(Xnoisy_r))
svd_Xnoisy_r <- svd(Xnoisy_r)
D <- diag(svd_Xnoisy_r$d)
U <- svd_Xnoisy_r$u
V <- svd_Xnoisy_r$v
threshold_singular <- median(svd_Xnoisy_r$d)*omega_found
up_to_sv <- length(svd_Xnoisy_r$d[svd_Xnoisy_r$d > threshold_singular])
D_filt <- D
diag(D_filt)[(up_to_sv + 1):length(diag(D_filt))] <- 0
Xnoisy_denoised <- U %*% D_filt %*% t(V)
dim(Xnoisy_denoised)
image(t(Xnoisy_denoised))
```

```{r,fig.dim = c(5,7),fig.align = 'center'}
denoised_matrix <- denoise_rectangular_matrix(Xnoisy_r)


image(t(Xnoisy_r))
image(t(denoised_matrix))
is.matrix("t")
```

The data included in this Vignette is derived from GSE42568. A dataset that contains  
one hundred and twenty one samples. One hundred and four derived from breast cancer samples
and seventeen derived from healthy breast mammary tissues. 

```{r,eval=FALSE}
library(SurvMap)

#Loading data.
data("GSE42568_Collapsed")
bool_filt <- pData_FRMA$pCh_Status == "NT"
Exp_NT <- GEO_Eset_Norm_frma_Collapsed[,bool_filt]
pData_NT <- pData_FRMA[bool_filt,]
Exp_T <- GEO_Eset_Norm_frma_Collapsed[,!bool_filt]
pData_T <- pData_FRMA[!bool_filt,]

#Generaling the healthy tissue model.

Exp_NT_f <- flatten_normal_tiss(Exp_NT)
class(Exp_NT_f)
Exp_NT_f_d <- denoise_rectangular_matrix(Exp_NT_f)
class(Exp_NT_f)

cox_all <- cox_all_genes(Exp_T,pData_T$pCh_DFS_T,pData_T$pCh_DFS_E)

#Creating the disease component matrix.

Disease_component <- generate_disease_component(GEO_Eset_Norm_frma_Collapsed,Exp_NT_f)

Disease_component_tumors <- Disease_component[,!bool_filt]

#Selecting genes.

variable_genes <- gene_selection(Disease_component_tumors,0.99) #Top variable genes in tumor samples.

survival_related_genes <- get_survival_related_genes(cox_all,c(0.005,0.995)) #Genes showing the stringest association  with disease-free survival in univariate proporciohal hazar models analysis. (cox.)

selected_genes <- unique(c(survival_related_genes,variable_genes))
```

```{r,eval=FALSE}
Ds_for_an <- Disease_component[selected_genes,]

#filt_data <- lp_norm_k_powers(Ds_for_an,p = 2,k = 1)

filter_function <- lp_norm_k_powers_surv(Ds_for_an,2,1,cox_all)

int_data <- get_intervals_One_D(Ds_for_an,filter_function,10,0.2)

#Find samples at each level.

#filter_function[filter_function >= int_data[[1]][1] & filter_function < int_data[[1]][2]]

#which(filter_function >= int_data[[3]][1] & filter_function < int_data[[3]][2])

#lapply(int_data,function(x,y) names(which(y >= x[1] & y < x[2])),filter_function)

number_of_bins <- 100

test_ds_for_ann <- Ds_for_an[,lapply(int_data,function(x,y) names(which(y >= x[1] & y < x[2])),filter_function)[[9]]]



length(test_ds_for_ann)


clust_lev(test_ds_for_ann,distance_type = "cor",optimal_clust_mode = "standard",n_bins_clust = 10)




level_dist <- as.dist(1-cor(test_ds_for_ann))

#level_dist <- dist(test_ds_for_ann,method = "euclidean")

level_max_dist <- max(level_dist)

level_hcluster_ouput <- hclust(level_dist,method="single")

cluster::silhouette(cutree(level_hcluster_ouput,4),level_dist)

length(level_hcluster_ouput$order)




for(i in 2:(length(level_hcluster_ouput$order)-1)){
  print(i)
  test <- silhouette(cutree(level_hcluster_ouput,i),level_dist)
  print(mean(test[,3]))
}

0.71-1.0
A strong structure has been found

0.51-0.70
A reasonable structure has been found

0.26-0.50
The structure is weak and could be artificial. Try additional methods of data analysis.

< 0.25
No substantial structure has been found

https://stats.stackexchange.com/questions/10540/how-to-interpret-mean-of-silhouette-plot
test <- silhouette(cutree(level_hcluster_ouput,103),level_dist)
plot(test)
mean(test[,3])
silhouette
cluster:::silhouette

help(silhouette)


level_hcluster_ouput$
plot(level_hcluster_ouput)
help("hclust")
heights <- level_hcluster_ouput$height
level_hcluster_ouput$order
level_hcluster_ouput$labels
hist(heights)

library(cluster)
plot(silhouette(cutree(level_hcluster_ouput, h=0.45),level_dist))


help("hclust")

bin_breaks <- seq(from=min(heights), to=level_max_dist, by=(level_max_dist - min(heights))/number_of_bins)

myhist <- hist(c(heights,level_max_dist), breaks=bin_breaks, plot=FALSE)
plot(myhist)
level_hcluster_ouput$height
myhist$counts == 0





z <- (myhist$counts == 0)

cutoff <- myhist$mids[min(which(z == TRUE))]

level_hcluster_ouput$height
cluster_indices_within_level <- as.vector( cutree(level_hcluster_ouput, h=cutoff) )



One_D_Mapper <- function(point_cloud,filter_data,n_intervals,percent_overlap){
  if(all(colnames(point_cloud) == names(filter_data))){
    min_val <- min(filter_data)
    max_val <- max(filter_data)
    interval_width <- (max_val - min_val)/n_intervals
    print(interval_width)
  }else{
    print("point cloud data columns and filter data values present different orders")
    }
}


One_D_Mapper(Ds_for_an,filt_data,5)

prod(5)



https://github.com/paultpearson/TDAmapper

library(graphics)
graphics::co.intervals
```


